import torch.nn as nn


class SimpleCNN(nn.Module):

	def __init__(self, alpha1=0.5, alpha2=0.5):
		super(SimpleCNN, self).__init__()
		self.bn1 = nn.BatchNorm1d(1)
		self.conv1 = nn.Conv1d(in_channels=1, out_channels=6, kernel_size=3, stride=1, padding=1)
		self.relu1 = nn.ReLU()
		self.conv2 = nn.Conv1d(in_channels=6, out_channels=12, kernel_size=3, stride=2, padding=1)
		self.relu2 = nn.ReLU()
		self.bn2 = nn.BatchNorm1d(12)
		self.conv3 = nn.Conv1d(12, 16, 3, stride=1, padding=1)
		self.relu3 = nn.ReLU()
		self.conv4 = nn.Conv1d(16, 24, 3, stride=2, padding=1)
		self.relu4 = nn.ReLU()
		self.bn3 = nn.BatchNorm1d(24)
		self.conv5 = nn.Conv1d(24, 28, 3, stride=1, padding=1)
		self.relu5 = nn.ReLU()
		self.conv6 = nn.Conv1d(28, 32, 3, stride=2, padding=1)
		self.relu6 = nn.ReLU()
		self.bn4 = nn.BatchNorm1d(32)
		self.conv7 = nn.Conv1d(32, 48, 3, stride=1, padding=1)
		self.relu7 = nn.ReLU()
		self.conv8 = nn.Conv1d(48, 64, 3, stride=2, padding=1)
		self.relu8 = nn.ReLU()
		self.bn5 = nn.BatchNorm1d(64)
		self.conv9 = nn.Conv1d(64, 98, 3, stride=1, padding=1)
		self.relu9 = nn.ReLU()
		self.conv10 = nn.Conv1d(98, 128, 3, stride=2, padding=1)
		self.relu10 = nn.ReLU()
		self.bn6 = nn.BatchNorm1d(128)
		self.bn7 = nn.BatchNorm1d(128 * 94)
		self.fc1 = nn.Linear(in_features=128 * 94, out_features=64)
		self.relu6 = nn.ReLU()
		self.bn8 = nn.BatchNorm1d(64)
		self.dropout1 = nn.Dropout(alpha1)
		self.dropout2 = nn.Dropout(alpha2)
		self.fc2 = nn.Linear(64, 5)
		self.softmax = nn.Softmax()

	def forward(self, x):
		x = self.bn1(x)
		x = self.relu1(self.conv1(x))
		x = self.relu2(self.conv2(x))
		x = self.bn2(x)
		x = self.relu3(self.conv3(x))
		x = self.relu4(self.conv4(x))
		x = self.bn3(x)
		x = self.relu5(self.conv5(x))
		x = self.relu6(self.conv6(x))
		x = self.bn4(x)
		x = self.relu7(self.conv7(x))
		x = self.relu8(self.conv8(x))
		x = self.bn5(x)
		x = self.relu9(self.conv9(x))
		x = self.relu10(self.conv10(x))
		x = self.bn6(x)
		x = self.dropout1(x)
		x = x.view(-1, 128 * 94)
		x = self.bn7(x)
		x = self.relu6(self.fc1(x))
		x = self.bn8(x)
		x = self.dropout2(x)
		x = self.fc2(x) #注意删除此行，没有报错
		x = self.softmax(x)
		return x
